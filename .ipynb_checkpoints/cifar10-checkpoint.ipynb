{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "os.environ['MKL_NUM_THREADS'] = str(8)\n",
    "os.environ['OMP_NUM_THREADS'] = str(8)\n",
    "OMP_NUM_THREADS=8\n",
    "MKL_NUM_THREADS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 layer randCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rand_Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rand_Net3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 512, 3,padding=1)\n",
    "        self.fc = nn.Linear(4096*2, 10)\n",
    "        for p in self.conv1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv2.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv3.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x, self.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.277\n",
      "[2,  2000] loss: 2.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-14:\n",
      "Traceback (most recent call last):\n",
      "Process Process-13:\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "KeyboardInterrupt\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "    r = index_queue.get()\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    return recv()\n",
      "    racquire()\n",
      "  File \"/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "KeyboardInterrupt\n",
      "    buf = self.recv_bytes()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-909142ac2493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-263fcae2f326>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net3 = rand_Net3()\n",
    "losses=[]\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, fcw = net3(inputs)\n",
    "            \n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, net3.parameters()), lr=0.001, momentum=0.9)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        l1 = torch.abs(fcw).sum()\n",
    "        #print('CRIT', criterion(outputs, labels))\n",
    "        #print('L1', l1)\n",
    "        \n",
    "        loss = criterion(outputs, labels) #+ 0.001*l1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 6000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "            #print('L1 = %s'%l1)\n",
    "    losses.append(loss.data[0])\n",
    "print('Finished Training')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 49 %\n",
      "Accuracy of   car : 51 %\n",
      "Accuracy of  bird : 30 %\n",
      "Accuracy of   cat : 25 %\n",
      "Accuracy of  deer : 39 %\n",
      "Accuracy of   dog : 41 %\n",
      "Accuracy of  frog : 59 %\n",
      "Accuracy of horse : 45 %\n",
      "Accuracy of  ship : 62 %\n",
      "Accuracy of truck : 54 %\n",
      "0.4595\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs,_ = net3(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(8):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print np.sum(class_correct)/np.sum(class_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rand_Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rand_Net4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 512, 3,padding=1)\n",
    "\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "        for p in self.conv1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv2.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv3.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv4.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)),kernel_size=2, stride=2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x, self.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.300\n",
      "[1,  4000] loss: 2.295\n",
      "[1,  6000] loss: 2.292\n",
      "[2,  2000] loss: 2.287\n",
      "[2,  4000] loss: 2.283\n",
      "[2,  6000] loss: 2.280\n",
      "[3,  2000] loss: 2.275\n",
      "[3,  4000] loss: 2.273\n",
      "[3,  6000] loss: 2.268\n",
      "[4,  2000] loss: 2.264\n",
      "[4,  4000] loss: 2.262\n",
      "[4,  6000] loss: 2.257\n",
      "[5,  2000] loss: 2.253\n",
      "[5,  4000] loss: 2.250\n",
      "[5,  6000] loss: 2.247\n",
      "[6,  2000] loss: 2.244\n",
      "[6,  4000] loss: 2.241\n",
      "[6,  6000] loss: 2.237\n",
      "[7,  2000] loss: 2.234\n",
      "[7,  4000] loss: 2.230\n",
      "[7,  6000] loss: 2.228\n",
      "[8,  2000] loss: 2.225\n",
      "[8,  4000] loss: 2.221\n",
      "[8,  6000] loss: 2.219\n",
      "[9,  2000] loss: 2.216\n",
      "[9,  4000] loss: 2.212\n",
      "[9,  6000] loss: 2.210\n",
      "[10,  2000] loss: 2.208\n",
      "[10,  4000] loss: 2.204\n",
      "[10,  6000] loss: 2.201\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117fdfed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net4 = rand_Net4()\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "losses=[]\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, fcw = net4(inputs)\n",
    "                \n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, net4.parameters()), lr=0.001, momentum=0.9)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #l1 = torch.abs(fcw).sum()\n",
    "        #print('CRIT', criterion(outputs, labels))\n",
    "        #print('L1', l1)\n",
    "        \n",
    "        loss = criterion(outputs, labels) #+ 0.001*l1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    losses.append(loss.data[0])\n",
    "plt.plot(losses)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 37 %\n",
      "Accuracy of   car : 29 %\n",
      "Accuracy of  bird :  8 %\n",
      "Accuracy of   cat :  6 %\n",
      "Accuracy of  deer : 19 %\n",
      "Accuracy of   dog : 14 %\n",
      "Accuracy of  frog : 59 %\n",
      "Accuracy of horse : 13 %\n",
      "Accuracy of  ship : 37 %\n",
      "Accuracy of truck : 60 %\n",
      "0.2859\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs,_ = net4(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(8):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print np.sum(class_correct)/np.sum(class_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vgg11 [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "   \n",
    "class rand_vgg11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rand_vgg11, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3,padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128*2, 3,padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128*2, 256*2, 3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(256*2, 256*2, 3,padding=1)\n",
    "                \n",
    "        self.conv5 = nn.Conv2d(256*2, 512*4, 3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(512*4, 512*4, 3,padding=1)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(512*4, 512*4, 3,padding=1)\n",
    "        self.conv8 = nn.Conv2d(512*4, 512*4, 3,padding=1)\n",
    "\n",
    "        self.fc = nn.Linear(512*4, 10)\n",
    "        for p in self.conv1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv2.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv3.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv4.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv5.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv6.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv7.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv8.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)),kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv6(x)),kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv8(x)),kernel_size=2, stride=2)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x, self.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netvgg11 = rand_vgg11()\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "losses=[]\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, fcw = netvgg11(inputs)\n",
    "                \n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, netvgg11.parameters()), lr=0.001, momentum=0.9)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #l1 = torch.abs(fcw).sum()\n",
    "        #print('CRIT', criterion(outputs, labels))\n",
    "        #print('L1', l1)\n",
    "        \n",
    "        loss = criterion(outputs, labels) #+ 0.001*l1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    losses.append(loss.data[0])\n",
    "plt.plot(losses)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane :  0 %\n",
      "Accuracy of   car :  0 %\n",
      "Accuracy of  bird :  0 %\n",
      "Accuracy of   cat :  0 %\n",
      "Accuracy of  deer : 100 %\n",
      "Accuracy of   dog :  0 %\n",
      "Accuracy of  frog :  0 %\n",
      "Accuracy of horse :  0 %\n",
      "Accuracy of  ship :  0 %\n",
      "Accuracy of truck :  0 %\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs,_ = netvgg11(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(8):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print np.sum(class_correct)/np.sum(class_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to pixel by pixel Lasso classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=7000,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=500,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute purely on pixels\n",
      "1\n",
      "-----\n",
      "Accuracy of plane : 43 %\n",
      "Accuracy of   car : 43 %\n",
      "Accuracy of  bird : 26 %\n",
      "Accuracy of   cat : 17 %\n",
      "Accuracy of  deer : 28 %\n",
      "Accuracy of   dog : 27 %\n",
      "Accuracy of  frog : 48 %\n",
      "Accuracy of horse : 48 %\n",
      "Accuracy of  ship : 53 %\n",
      "Accuracy of truck : 41 %\n",
      "0.3804\n",
      "-----\n",
      "Accuracy of plane : 46 %\n",
      "Accuracy of   car : 46 %\n",
      "Accuracy of  bird : 33 %\n",
      "Accuracy of   cat : 21 %\n",
      "Accuracy of  deer : 32 %\n",
      "Accuracy of   dog : 36 %\n",
      "Accuracy of  frog : 51 %\n",
      "Accuracy of horse : 45 %\n",
      "Accuracy of  ship : 66 %\n",
      "Accuracy of truck : 51 %\n",
      "0.4368\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model,metrics,preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print('compute purely on pixels')\n",
    "for ty,data in enumerate(trainloader):\n",
    "    images, labels = data\n",
    "    for alpha in [1]:#np.logspace(0,3,10):\n",
    "        print alpha\n",
    "        reg = linear_model.LogisticRegression(penalty='l1',solver ='saga',C = alpha,n_jobs=6,max_iter=3)\n",
    "        rf = RandomForestClassifier(n_estimators = 300,n_jobs=6)\n",
    "        reg.fit(images[:,:,:,:].numpy().reshape(7000,-1) , labels) \n",
    "        rf.fit(images[:,:,:,:].numpy().reshape(7000,-1) , labels) \n",
    "    for mod in [reg,rf]:\n",
    "        print (\"-----\")\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        for a,data in enumerate(testloader):\n",
    "            imagesp, labelsp = data\n",
    "            predicted = mod.predict(imagesp[:,:,:,:].numpy().reshape(500,-1))\n",
    "            c = (predicted == labelsp).squeeze()\n",
    "            for i in range(500):\n",
    "                label = labelsp[i]\n",
    "                class_correct[label] += c[i]\n",
    "                class_total[label] += 1\n",
    "            if a>=4:\n",
    "                break \n",
    "        for i in range(10):\n",
    "            print('Accuracy of %5s : %2d %%' % (\n",
    "                classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "        print np.sum(class_correct)/np.sum(class_total)\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
