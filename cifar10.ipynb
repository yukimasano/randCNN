{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "os.environ['MKL_NUM_THREADS'] = \"8\"\n",
    "os.environ['OMP_NUM_THREADS'] = \"8\"\n",
    "OMP_NUM_THREADS=8\n",
    "MKL_NUM_THREADS=8\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
    "    lr = 0.05 * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def train_eval(net,n_epochs=50):\n",
    "    losses=[]\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, fcw = net(inputs)\n",
    "\n",
    "            #optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9)\n",
    "            \n",
    "            optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), \n",
    "                                  lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "            adjust_learning_rate(optimizer, epoch)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            l1 = torch.abs(fcw).sum()\n",
    "            #print('CRIT', criterion(outputs, labels))\n",
    "            #print('L1', l1)\n",
    "\n",
    "            loss = criterion(outputs, labels) #+ 0.001*l1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                losses.append(running_loss/ 2000)\n",
    "                running_loss = 0.0\n",
    "                #print('L1 = %s'%l1)\n",
    "        \n",
    "    print('Finished Training')\n",
    "    plt.plot(losses)\n",
    "\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs,_ = net(Variable(images).cuda())\n",
    "        _, predicted = torch.max(outputs.data.cpu(), 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(8):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i]\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    print np.sum(class_correct)/np.sum(class_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 layer randCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rand_Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rand_Net3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 512, 3,padding=1)\n",
    "        self.fc = nn.Linear(4096*2, 10)\n",
    "        for p in self.conv1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv2.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv3.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x, self.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.985\n",
      "[2,  2000] loss: 1.732\n",
      "[3,  2000] loss: 1.642\n",
      "[4,  2000] loss: 1.598\n",
      "[5,  2000] loss: 1.560\n",
      "[6,  2000] loss: 1.532\n",
      "[7,  2000] loss: 1.524\n",
      "[8,  2000] loss: 1.497\n",
      "[9,  2000] loss: 1.485\n",
      "[10,  2000] loss: 1.476\n",
      "[11,  2000] loss: 1.470\n",
      "[12,  2000] loss: 1.464\n",
      "[13,  2000] loss: 1.459\n",
      "[14,  2000] loss: 1.448\n",
      "[15,  2000] loss: 1.441\n",
      "[16,  2000] loss: 1.441\n",
      "[17,  2000] loss: 1.433\n",
      "[18,  2000] loss: 1.431\n",
      "[19,  2000] loss: 1.439\n",
      "[20,  2000] loss: 1.426\n",
      "[21,  2000] loss: 1.431\n",
      "[22,  2000] loss: 1.423\n",
      "[23,  2000] loss: 1.414\n",
      "[24,  2000] loss: 1.419\n",
      "[25,  2000] loss: 1.424\n",
      "[26,  2000] loss: 1.425\n",
      "[27,  2000] loss: 1.426\n",
      "[28,  2000] loss: 1.409\n",
      "[29,  2000] loss: 1.423\n",
      "[30,  2000] loss: 1.423\n",
      "[31,  2000] loss: 1.318\n",
      "[32,  2000] loss: 1.308\n",
      "[33,  2000] loss: 1.318\n",
      "[34,  2000] loss: 1.323\n",
      "[35,  2000] loss: 1.317\n",
      "[36,  2000] loss: 1.314\n",
      "[37,  2000] loss: 1.321\n",
      "[38,  2000] loss: 1.321\n",
      "[39,  2000] loss: 1.321\n",
      "[40,  2000] loss: 1.315\n",
      "[41,  2000] loss: 1.317\n",
      "[42,  2000] loss: 1.318\n",
      "[43,  2000] loss: 1.314\n",
      "[44,  2000] loss: 1.315\n",
      "[45,  2000] loss: 1.328\n",
      "[46,  2000] loss: 1.321\n",
      "[47,  2000] loss: 1.323\n",
      "[48,  2000] loss: 1.317\n",
      "[49,  2000] loss: 1.322\n",
      "[50,  2000] loss: 1.318\n",
      "[51,  2000] loss: 1.327\n",
      "[52,  2000] loss: 1.323\n",
      "[53,  2000] loss: 1.318\n",
      "[54,  2000] loss: 1.320\n",
      "[55,  2000] loss: 1.324\n",
      "[56,  2000] loss: 1.320\n",
      "[57,  2000] loss: 1.317\n",
      "[58,  2000] loss: 1.324\n",
      "[59,  2000] loss: 1.325\n",
      "[60,  2000] loss: 1.321\n",
      "[61,  2000] loss: 1.284\n",
      "[62,  2000] loss: 1.285\n",
      "[63,  2000] loss: 1.286\n",
      "[64,  2000] loss: 1.290\n",
      "[65,  2000] loss: 1.289\n",
      "[66,  2000] loss: 1.286\n",
      "[67,  2000] loss: 1.287\n",
      "[68,  2000] loss: 1.286\n",
      "[69,  2000] loss: 1.290\n",
      "[70,  2000] loss: 1.286\n",
      "[71,  2000] loss: 1.285\n",
      "[72,  2000] loss: 1.292\n",
      "[73,  2000] loss: 1.285\n",
      "[74,  2000] loss: 1.288\n",
      "[75,  2000] loss: 1.284\n",
      "[76,  2000] loss: 1.288\n",
      "[77,  2000] loss: 1.288\n",
      "[78,  2000] loss: 1.295\n",
      "[79,  2000] loss: 1.292\n",
      "[80,  2000] loss: 1.286\n",
      "[81,  2000] loss: 1.284\n",
      "[82,  2000] loss: 1.289\n",
      "[83,  2000] loss: 1.292\n",
      "[84,  2000] loss: 1.296\n",
      "[85,  2000] loss: 1.294\n",
      "[86,  2000] loss: 1.291\n",
      "[87,  2000] loss: 1.292\n",
      "[88,  2000] loss: 1.289\n",
      "[89,  2000] loss: 1.288\n",
      "[90,  2000] loss: 1.285\n",
      "[91,  2000] loss: 1.270\n",
      "[92,  2000] loss: 1.269\n",
      "[93,  2000] loss: 1.273\n",
      "[94,  2000] loss: 1.273\n",
      "[95,  2000] loss: 1.277\n",
      "[96,  2000] loss: 1.275\n",
      "[97,  2000] loss: 1.273\n",
      "[98,  2000] loss: 1.268\n",
      "[99,  2000] loss: 1.275\n",
      "[100,  2000] loss: 1.274\n",
      "Finished Training\n",
      "Accuracy of plane : 70 %\n",
      "Accuracy of   car : 82 %\n",
      "Accuracy of  bird : 36 %\n",
      "Accuracy of   cat : 40 %\n",
      "Accuracy of  deer : 56 %\n",
      "Accuracy of   dog : 55 %\n",
      "Accuracy of  frog : 70 %\n",
      "Accuracy of horse : 65 %\n",
      "Accuracy of  ship : 69 %\n",
      "Accuracy of truck : 59 %\n",
      "0.6068\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dfc639fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net3 = rand_Net3()\n",
    "net3.cuda()\n",
    "train_eval(net3,n_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with different random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rand_Net3_lap(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rand_Net3_lap, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 512, 3,padding=1)\n",
    "        self.fc = nn.Linear(4096*2, 10)\n",
    "        for p in self.conv1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv2.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv3.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        for k in range(self.conv1.weight.data.shape[1]):\n",
    "            self.conv1.weight.data[k,0,:,:] = torch.FloatTensor(np.matmul(np.random.laplace(0,1./3,(3,1)),np.random.laplace(0,1./3,(1,3))))\n",
    "        for k in range(self.conv2.weight.data.shape[1]):\n",
    "            self.conv2.weight.data[k,0,:,:] = torch.FloatTensor(np.matmul(np.random.laplace(0,1./3,(3,1)),np.random.laplace(0,1./3,(1,3))))\n",
    "        for k in range(self.conv3.weight.data.shape[1]):\n",
    "            self.conv3.weight.data[k,0,:,:] = torch.FloatTensor(np.matmul(np.random.laplace(0,1./3,(3,1)),np.random.laplace(0,1./3,(1,3))))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x, self.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.120\n",
      "[2,  2000] loss: 1.803\n",
      "[3,  2000] loss: 1.709\n",
      "[4,  2000] loss: 1.659\n",
      "[5,  2000] loss: 1.622\n",
      "[6,  2000] loss: 1.594\n",
      "[7,  2000] loss: 1.566\n",
      "[8,  2000] loss: 1.558\n",
      "[9,  2000] loss: 1.542\n",
      "[10,  2000] loss: 1.535\n",
      "[11,  2000] loss: 1.530\n",
      "[12,  2000] loss: 1.509\n",
      "[13,  2000] loss: 1.513\n",
      "[14,  2000] loss: 1.498\n",
      "[15,  2000] loss: 1.501\n",
      "[16,  2000] loss: 1.489\n",
      "[17,  2000] loss: 1.497\n",
      "[18,  2000] loss: 1.490\n",
      "[19,  2000] loss: 1.490\n",
      "[20,  2000] loss: 1.473\n",
      "[21,  2000] loss: 1.482\n",
      "[22,  2000] loss: 1.490\n",
      "[23,  2000] loss: 1.470\n",
      "[24,  2000] loss: 1.490\n",
      "[25,  2000] loss: 1.480\n",
      "[26,  2000] loss: 1.470\n",
      "[27,  2000] loss: 1.475\n",
      "[28,  2000] loss: 1.469\n",
      "[29,  2000] loss: 1.470\n",
      "[30,  2000] loss: 1.471\n",
      "[31,  2000] loss: 1.325\n",
      "[32,  2000] loss: 1.318\n",
      "[33,  2000] loss: 1.314\n",
      "[34,  2000] loss: 1.317\n",
      "[35,  2000] loss: 1.325\n",
      "[36,  2000] loss: 1.320\n",
      "[37,  2000] loss: 1.324\n",
      "[38,  2000] loss: 1.324\n",
      "[39,  2000] loss: 1.324\n",
      "[40,  2000] loss: 1.320\n",
      "[41,  2000] loss: 1.320\n",
      "[42,  2000] loss: 1.327\n",
      "[43,  2000] loss: 1.318\n",
      "[44,  2000] loss: 1.320\n",
      "[45,  2000] loss: 1.318\n",
      "[46,  2000] loss: 1.320\n",
      "[47,  2000] loss: 1.326\n",
      "[48,  2000] loss: 1.321\n",
      "[49,  2000] loss: 1.318\n",
      "[50,  2000] loss: 1.323\n",
      "[51,  2000] loss: 1.321\n",
      "[52,  2000] loss: 1.317\n",
      "[53,  2000] loss: 1.325\n",
      "[54,  2000] loss: 1.323\n",
      "[55,  2000] loss: 1.321\n",
      "[56,  2000] loss: 1.318\n",
      "[57,  2000] loss: 1.328\n",
      "[58,  2000] loss: 1.321\n",
      "[59,  2000] loss: 1.327\n",
      "[60,  2000] loss: 1.318\n",
      "[61,  2000] loss: 1.277\n",
      "[62,  2000] loss: 1.278\n",
      "[63,  2000] loss: 1.279\n",
      "[64,  2000] loss: 1.281\n",
      "[65,  2000] loss: 1.279\n",
      "[66,  2000] loss: 1.282\n",
      "[67,  2000] loss: 1.276\n",
      "[68,  2000] loss: 1.285\n",
      "[69,  2000] loss: 1.275\n",
      "[70,  2000] loss: 1.288\n",
      "[71,  2000] loss: 1.276\n",
      "[72,  2000] loss: 1.281\n",
      "[73,  2000] loss: 1.281\n",
      "[74,  2000] loss: 1.284\n",
      "[75,  2000] loss: 1.280\n",
      "[76,  2000] loss: 1.280\n",
      "[77,  2000] loss: 1.277\n",
      "[78,  2000] loss: 1.284\n",
      "[79,  2000] loss: 1.280\n",
      "[80,  2000] loss: 1.274\n",
      "[81,  2000] loss: 1.284\n",
      "[82,  2000] loss: 1.281\n",
      "[83,  2000] loss: 1.279\n",
      "[84,  2000] loss: 1.281\n",
      "[85,  2000] loss: 1.285\n",
      "[86,  2000] loss: 1.285\n",
      "[87,  2000] loss: 1.279\n",
      "[88,  2000] loss: 1.279\n",
      "[89,  2000] loss: 1.273\n",
      "[90,  2000] loss: 1.282\n",
      "[91,  2000] loss: 1.264\n",
      "[92,  2000] loss: 1.261\n",
      "[93,  2000] loss: 1.263\n",
      "[94,  2000] loss: 1.261\n",
      "[95,  2000] loss: 1.261\n",
      "[96,  2000] loss: 1.263\n",
      "[97,  2000] loss: 1.257\n",
      "[98,  2000] loss: 1.264\n",
      "[99,  2000] loss: 1.268\n",
      "[100,  2000] loss: 1.267\n",
      "[101,  2000] loss: 1.264\n",
      "[102,  2000] loss: 1.263\n",
      "[103,  2000] loss: 1.269\n",
      "[104,  2000] loss: 1.262\n",
      "[105,  2000] loss: 1.262\n",
      "[106,  2000] loss: 1.261\n",
      "[107,  2000] loss: 1.259\n",
      "[108,  2000] loss: 1.267\n",
      "[109,  2000] loss: 1.265\n"
     ]
    }
   ],
   "source": [
    "net3l = rand_Net3_lap()\n",
    "net3l.cuda()\n",
    "train_eval(net3l,n_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/cdt17/yuki/.conda/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.773\n",
      "[2,  2000] loss: 1.470\n",
      "[3,  2000] loss: 1.392\n",
      "[4,  2000] loss: 1.348\n",
      "[5,  2000] loss: 1.306\n",
      "[6,  2000] loss: 1.287\n",
      "[7,  2000] loss: 1.260\n",
      "[8,  2000] loss: 1.251\n",
      "[9,  2000] loss: 1.238\n",
      "[10,  2000] loss: 1.225\n",
      "[11,  2000] loss: 1.218\n",
      "[12,  2000] loss: 1.217\n",
      "[13,  2000] loss: 1.202\n",
      "[14,  2000] loss: 1.199\n",
      "[15,  2000] loss: 1.195\n",
      "[16,  2000] loss: 1.186\n",
      "[17,  2000] loss: 1.187\n",
      "[18,  2000] loss: 1.180\n",
      "[19,  2000] loss: 1.180\n",
      "[20,  2000] loss: 1.178\n",
      "[21,  2000] loss: 1.171\n",
      "[22,  2000] loss: 1.174\n",
      "[23,  2000] loss: 1.176\n",
      "[24,  2000] loss: 1.165\n",
      "[25,  2000] loss: 1.166\n",
      "[26,  2000] loss: 1.162\n",
      "[27,  2000] loss: 1.163\n",
      "[28,  2000] loss: 1.168\n",
      "[29,  2000] loss: 1.164\n",
      "[30,  2000] loss: 1.164\n",
      "[31,  2000] loss: 1.145\n",
      "[32,  2000] loss: 1.148\n",
      "[33,  2000] loss: 1.139\n",
      "[34,  2000] loss: 1.146\n",
      "[35,  2000] loss: 1.146\n",
      "[36,  2000] loss: 1.148\n",
      "[37,  2000] loss: 1.145\n",
      "[38,  2000] loss: 1.146\n",
      "[39,  2000] loss: 1.141\n",
      "[40,  2000] loss: 1.142\n",
      "[41,  2000] loss: 1.138\n",
      "[42,  2000] loss: 1.139\n",
      "[43,  2000] loss: 1.143\n",
      "[44,  2000] loss: 1.143\n",
      "[45,  2000] loss: 1.143\n",
      "[46,  2000] loss: 1.147\n",
      "[47,  2000] loss: 1.142\n",
      "[48,  2000] loss: 1.142\n",
      "[49,  2000] loss: 1.139\n",
      "[50,  2000] loss: 1.145\n",
      "[51,  2000] loss: 1.141\n",
      "[52,  2000] loss: 1.143\n",
      "[53,  2000] loss: 1.140\n",
      "[54,  2000] loss: 1.142\n",
      "[55,  2000] loss: 1.140\n",
      "[56,  2000] loss: 1.139\n",
      "[57,  2000] loss: 1.142\n",
      "[58,  2000] loss: 1.144\n",
      "[59,  2000] loss: 1.138\n",
      "[60,  2000] loss: 1.136\n",
      "[61,  2000] loss: 1.130\n",
      "[62,  2000] loss: 1.133\n",
      "[63,  2000] loss: 1.136\n",
      "[64,  2000] loss: 1.129\n",
      "[65,  2000] loss: 1.132\n",
      "[66,  2000] loss: 1.134\n",
      "[67,  2000] loss: 1.132\n",
      "[68,  2000] loss: 1.135\n",
      "[69,  2000] loss: 1.133\n",
      "[70,  2000] loss: 1.133\n",
      "[71,  2000] loss: 1.134\n",
      "[72,  2000] loss: 1.131\n",
      "[73,  2000] loss: 1.129\n",
      "[74,  2000] loss: 1.131\n",
      "[75,  2000] loss: 1.132\n",
      "[76,  2000] loss: 1.133\n",
      "[77,  2000] loss: 1.136\n",
      "[78,  2000] loss: 1.131\n",
      "[79,  2000] loss: 1.130\n",
      "[80,  2000] loss: 1.135\n",
      "[81,  2000] loss: 1.132\n",
      "[82,  2000] loss: 1.129\n",
      "[83,  2000] loss: 1.129\n",
      "[84,  2000] loss: 1.131\n",
      "[85,  2000] loss: 1.134\n",
      "[86,  2000] loss: 1.129\n",
      "[87,  2000] loss: 1.132\n",
      "[88,  2000] loss: 1.132\n",
      "[89,  2000] loss: 1.130\n",
      "[90,  2000] loss: 1.132\n",
      "[91,  2000] loss: 1.127\n",
      "[92,  2000] loss: 1.129\n",
      "[93,  2000] loss: 1.125\n",
      "[94,  2000] loss: 1.130\n",
      "[95,  2000] loss: 1.131\n",
      "[96,  2000] loss: 1.129\n",
      "[97,  2000] loss: 1.125\n",
      "[98,  2000] loss: 1.128\n",
      "[99,  2000] loss: 1.130\n",
      "[100,  2000] loss: 1.126\n",
      "[101,  2000] loss: 1.127\n",
      "[102,  2000] loss: 1.134\n",
      "[103,  2000] loss: 1.127\n",
      "[104,  2000] loss: 1.130\n",
      "[105,  2000] loss: 1.133\n",
      "[106,  2000] loss: 1.127\n",
      "[107,  2000] loss: 1.129\n",
      "[108,  2000] loss: 1.128\n",
      "[109,  2000] loss: 1.127\n",
      "[110,  2000] loss: 1.126\n",
      "[111,  2000] loss: 1.129\n",
      "[112,  2000] loss: 1.124\n",
      "[113,  2000] loss: 1.127\n",
      "[114,  2000] loss: 1.131\n",
      "[115,  2000] loss: 1.131\n",
      "[116,  2000] loss: 1.130\n",
      "[117,  2000] loss: 1.129\n",
      "[118,  2000] loss: 1.124\n",
      "[119,  2000] loss: 1.126\n",
      "[120,  2000] loss: 1.125\n",
      "[121,  2000] loss: 1.127\n",
      "[122,  2000] loss: 1.124\n",
      "[123,  2000] loss: 1.122\n",
      "[124,  2000] loss: 1.123\n",
      "[125,  2000] loss: 1.126\n",
      "[126,  2000] loss: 1.125\n",
      "[127,  2000] loss: 1.127\n",
      "[128,  2000] loss: 1.129\n",
      "[129,  2000] loss: 1.123\n",
      "[130,  2000] loss: 1.125\n",
      "[131,  2000] loss: 1.132\n",
      "[132,  2000] loss: 1.125\n",
      "[133,  2000] loss: 1.122\n",
      "[134,  2000] loss: 1.130\n",
      "[135,  2000] loss: 1.124\n",
      "[136,  2000] loss: 1.124\n",
      "[137,  2000] loss: 1.123\n",
      "[138,  2000] loss: 1.124\n",
      "[139,  2000] loss: 1.124\n",
      "[140,  2000] loss: 1.125\n",
      "[141,  2000] loss: 1.126\n",
      "[142,  2000] loss: 1.126\n",
      "[143,  2000] loss: 1.128\n",
      "[144,  2000] loss: 1.127\n",
      "[145,  2000] loss: 1.129\n",
      "[146,  2000] loss: 1.122\n",
      "[147,  2000] loss: 1.121\n",
      "[148,  2000] loss: 1.122\n",
      "[149,  2000] loss: 1.123\n",
      "[150,  2000] loss: 1.126\n",
      "[151,  2000] loss: 1.124\n",
      "[152,  2000] loss: 1.129\n",
      "[153,  2000] loss: 1.124\n",
      "[154,  2000] loss: 1.125\n",
      "[155,  2000] loss: 1.129\n",
      "[156,  2000] loss: 1.122\n",
      "[157,  2000] loss: 1.126\n",
      "[158,  2000] loss: 1.125\n",
      "[159,  2000] loss: 1.129\n",
      "[160,  2000] loss: 1.125\n",
      "[161,  2000] loss: 1.121\n",
      "[162,  2000] loss: 1.124\n",
      "[163,  2000] loss: 1.127\n",
      "[164,  2000] loss: 1.127\n",
      "[165,  2000] loss: 1.127\n",
      "[166,  2000] loss: 1.125\n",
      "[167,  2000] loss: 1.127\n",
      "[168,  2000] loss: 1.122\n",
      "[169,  2000] loss: 1.126\n",
      "[170,  2000] loss: 1.126\n",
      "[171,  2000] loss: 1.121\n",
      "[172,  2000] loss: 1.123\n",
      "[173,  2000] loss: 1.126\n",
      "[174,  2000] loss: 1.122\n",
      "[175,  2000] loss: 1.122\n",
      "[176,  2000] loss: 1.124\n",
      "[177,  2000] loss: 1.127\n",
      "[178,  2000] loss: 1.127\n",
      "[179,  2000] loss: 1.121\n",
      "[180,  2000] loss: 1.124\n",
      "[181,  2000] loss: 1.126\n",
      "[182,  2000] loss: 1.123\n",
      "[183,  2000] loss: 1.125\n",
      "[184,  2000] loss: 1.127\n",
      "[185,  2000] loss: 1.125\n",
      "[186,  2000] loss: 1.123\n",
      "[187,  2000] loss: 1.125\n",
      "[188,  2000] loss: 1.120\n",
      "[189,  2000] loss: 1.126\n",
      "[190,  2000] loss: 1.124\n",
      "[191,  2000] loss: 1.126\n",
      "[192,  2000] loss: 1.129\n",
      "[193,  2000] loss: 1.130\n",
      "[194,  2000] loss: 1.119\n",
      "[195,  2000] loss: 1.127\n",
      "[196,  2000] loss: 1.130\n",
      "[197,  2000] loss: 1.126\n",
      "[198,  2000] loss: 1.124\n",
      "[199,  2000] loss: 1.122\n",
      "[200,  2000] loss: 1.125\n",
      "Finished Training\n",
      "Accuracy of plane : 62 %\n",
      "Accuracy of   car : 71 %\n",
      "Accuracy of  bird : 47 %\n",
      "Accuracy of   cat : 42 %\n",
      "Accuracy of  deer : 52 %\n",
      "Accuracy of   dog : 49 %\n",
      "Accuracy of  frog : 70 %\n",
      "Accuracy of horse : 67 %\n",
      "Accuracy of  ship : 76 %\n",
      "Accuracy of truck : 67 %\n",
      "0.6084\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dfc6dee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net3l_ = rand_Net3_lap()\n",
    "net3l_.cuda()\n",
    "train_eval(net3l_,n_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rand_Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rand_Net4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32*2, 3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(32*2, 64*2, 3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(64*2, 128*2, 3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(128*2, 512*2, 3,padding=1)\n",
    "\n",
    "        self.fc = nn.Linear(2048*2, 10)\n",
    "        for p in self.conv1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv2.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv3.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv4.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)),kernel_size=2, stride=2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x, self.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.099\n",
      "[2,  2000] loss: 1.878\n",
      "[3,  2000] loss: 1.801\n",
      "[4,  2000] loss: 1.762\n",
      "[5,  2000] loss: 1.728\n",
      "[6,  2000] loss: 1.706\n",
      "[7,  2000] loss: 1.688\n",
      "[8,  2000] loss: 1.683\n",
      "[9,  2000] loss: 1.669\n",
      "[10,  2000] loss: 1.660\n",
      "[11,  2000] loss: 1.652\n",
      "[12,  2000] loss: 1.644\n",
      "[13,  2000] loss: 1.637\n",
      "[14,  2000] loss: 1.629\n",
      "[15,  2000] loss: 1.626\n",
      "[16,  2000] loss: 1.626\n",
      "[17,  2000] loss: 1.621\n",
      "[18,  2000] loss: 1.619\n",
      "[19,  2000] loss: 1.615\n",
      "[20,  2000] loss: 1.613\n",
      "[21,  2000] loss: 1.617\n",
      "[22,  2000] loss: 1.611\n",
      "[23,  2000] loss: 1.611\n",
      "[24,  2000] loss: 1.603\n",
      "[25,  2000] loss: 1.606\n",
      "[26,  2000] loss: 1.608\n",
      "[27,  2000] loss: 1.604\n",
      "[28,  2000] loss: 1.604\n",
      "[29,  2000] loss: 1.606\n",
      "[30,  2000] loss: 1.604\n",
      "[31,  2000] loss: 1.592\n",
      "[32,  2000] loss: 1.594\n",
      "[33,  2000] loss: 1.594\n",
      "[34,  2000] loss: 1.586\n",
      "[35,  2000] loss: 1.595\n",
      "[36,  2000] loss: 1.591\n",
      "[37,  2000] loss: 1.590\n",
      "[38,  2000] loss: 1.590\n",
      "[39,  2000] loss: 1.595\n",
      "[40,  2000] loss: 1.590\n",
      "[41,  2000] loss: 1.593\n",
      "[42,  2000] loss: 1.589\n",
      "[43,  2000] loss: 1.597\n",
      "[44,  2000] loss: 1.594\n",
      "[45,  2000] loss: 1.591\n",
      "[46,  2000] loss: 1.585\n",
      "[47,  2000] loss: 1.593\n",
      "[48,  2000] loss: 1.589\n",
      "[49,  2000] loss: 1.594\n",
      "[50,  2000] loss: 1.589\n",
      "[51,  2000] loss: 1.591\n",
      "[52,  2000] loss: 1.589\n",
      "[53,  2000] loss: 1.588\n",
      "[54,  2000] loss: 1.586\n",
      "[55,  2000] loss: 1.586\n",
      "[56,  2000] loss: 1.592\n",
      "[57,  2000] loss: 1.592\n",
      "[58,  2000] loss: 1.592\n",
      "[59,  2000] loss: 1.590\n",
      "[60,  2000] loss: 1.587\n",
      "[61,  2000] loss: 1.582\n",
      "[62,  2000] loss: 1.583\n",
      "[63,  2000] loss: 1.581\n",
      "[64,  2000] loss: 1.588\n",
      "[65,  2000] loss: 1.590\n",
      "[66,  2000] loss: 1.584\n",
      "[67,  2000] loss: 1.582\n",
      "[68,  2000] loss: 1.589\n",
      "[69,  2000] loss: 1.584\n",
      "[70,  2000] loss: 1.586\n",
      "[71,  2000] loss: 1.578\n",
      "[72,  2000] loss: 1.586\n",
      "[73,  2000] loss: 1.585\n",
      "[74,  2000] loss: 1.583\n",
      "[75,  2000] loss: 1.586\n",
      "[76,  2000] loss: 1.584\n",
      "[77,  2000] loss: 1.585\n",
      "[78,  2000] loss: 1.592\n",
      "[79,  2000] loss: 1.585\n",
      "[80,  2000] loss: 1.581\n",
      "[81,  2000] loss: 1.588\n",
      "[82,  2000] loss: 1.585\n",
      "[83,  2000] loss: 1.584\n",
      "[84,  2000] loss: 1.582\n",
      "[85,  2000] loss: 1.582\n",
      "[86,  2000] loss: 1.588\n",
      "[87,  2000] loss: 1.583\n",
      "[88,  2000] loss: 1.585\n",
      "[89,  2000] loss: 1.583\n",
      "[90,  2000] loss: 1.582\n",
      "[91,  2000] loss: 1.583\n",
      "[92,  2000] loss: 1.584\n",
      "[93,  2000] loss: 1.584\n",
      "[94,  2000] loss: 1.584\n",
      "[95,  2000] loss: 1.584\n",
      "[96,  2000] loss: 1.585\n",
      "[97,  2000] loss: 1.585\n",
      "[98,  2000] loss: 1.583\n",
      "[99,  2000] loss: 1.580\n",
      "[100,  2000] loss: 1.578\n",
      "[101,  2000] loss: 1.582\n",
      "[102,  2000] loss: 1.583\n",
      "[103,  2000] loss: 1.584\n",
      "[104,  2000] loss: 1.583\n",
      "[105,  2000] loss: 1.581\n",
      "[106,  2000] loss: 1.585\n",
      "[107,  2000] loss: 1.584\n",
      "[108,  2000] loss: 1.583\n",
      "[109,  2000] loss: 1.578\n",
      "[110,  2000] loss: 1.585\n",
      "[111,  2000] loss: 1.583\n",
      "[112,  2000] loss: 1.579\n",
      "[113,  2000] loss: 1.585\n",
      "[114,  2000] loss: 1.579\n",
      "[115,  2000] loss: 1.583\n",
      "[116,  2000] loss: 1.582\n",
      "[117,  2000] loss: 1.584\n",
      "[118,  2000] loss: 1.582\n",
      "[119,  2000] loss: 1.585\n",
      "[120,  2000] loss: 1.583\n",
      "[121,  2000] loss: 1.580\n",
      "[122,  2000] loss: 1.576\n",
      "[123,  2000] loss: 1.578\n",
      "[124,  2000] loss: 1.580\n",
      "[125,  2000] loss: 1.579\n",
      "[126,  2000] loss: 1.585\n",
      "[127,  2000] loss: 1.577\n",
      "[128,  2000] loss: 1.580\n",
      "[129,  2000] loss: 1.579\n",
      "[130,  2000] loss: 1.576\n",
      "[131,  2000] loss: 1.584\n",
      "[132,  2000] loss: 1.579\n",
      "[133,  2000] loss: 1.579\n",
      "[134,  2000] loss: 1.581\n",
      "[135,  2000] loss: 1.580\n",
      "[136,  2000] loss: 1.580\n",
      "[137,  2000] loss: 1.585\n",
      "[138,  2000] loss: 1.581\n",
      "[139,  2000] loss: 1.578\n",
      "[140,  2000] loss: 1.580\n",
      "[141,  2000] loss: 1.584\n",
      "[142,  2000] loss: 1.579\n",
      "[143,  2000] loss: 1.582\n",
      "[144,  2000] loss: 1.586\n",
      "[145,  2000] loss: 1.583\n",
      "[146,  2000] loss: 1.584\n",
      "[147,  2000] loss: 1.580\n",
      "[148,  2000] loss: 1.584\n",
      "[149,  2000] loss: 1.578\n",
      "[150,  2000] loss: 1.582\n",
      "[151,  2000] loss: 1.581\n",
      "[152,  2000] loss: 1.578\n",
      "[153,  2000] loss: 1.578\n",
      "[154,  2000] loss: 1.581\n",
      "[155,  2000] loss: 1.577\n",
      "[156,  2000] loss: 1.580\n",
      "[157,  2000] loss: 1.578\n",
      "[158,  2000] loss: 1.578\n",
      "[159,  2000] loss: 1.582\n",
      "[160,  2000] loss: 1.583\n",
      "[161,  2000] loss: 1.581\n",
      "[162,  2000] loss: 1.576\n",
      "[163,  2000] loss: 1.579\n",
      "[164,  2000] loss: 1.581\n",
      "[165,  2000] loss: 1.580\n",
      "[166,  2000] loss: 1.581\n",
      "[167,  2000] loss: 1.582\n",
      "[168,  2000] loss: 1.580\n",
      "[169,  2000] loss: 1.581\n",
      "[170,  2000] loss: 1.583\n",
      "[171,  2000] loss: 1.583\n",
      "[172,  2000] loss: 1.579\n",
      "[173,  2000] loss: 1.583\n",
      "[174,  2000] loss: 1.582\n",
      "[175,  2000] loss: 1.580\n",
      "[176,  2000] loss: 1.574\n",
      "[177,  2000] loss: 1.581\n",
      "[178,  2000] loss: 1.588\n",
      "[179,  2000] loss: 1.577\n",
      "[180,  2000] loss: 1.579\n",
      "[181,  2000] loss: 1.579\n",
      "[182,  2000] loss: 1.577\n",
      "[183,  2000] loss: 1.581\n",
      "[184,  2000] loss: 1.581\n",
      "[185,  2000] loss: 1.581\n",
      "[186,  2000] loss: 1.580\n",
      "[187,  2000] loss: 1.577\n",
      "[188,  2000] loss: 1.583\n",
      "[189,  2000] loss: 1.580\n",
      "[190,  2000] loss: 1.581\n",
      "[191,  2000] loss: 1.576\n",
      "[192,  2000] loss: 1.576\n",
      "[193,  2000] loss: 1.577\n",
      "[194,  2000] loss: 1.583\n",
      "[195,  2000] loss: 1.580\n",
      "[196,  2000] loss: 1.582\n",
      "[197,  2000] loss: 1.579\n",
      "[198,  2000] loss: 1.578\n",
      "[199,  2000] loss: 1.582\n",
      "[200,  2000] loss: 1.578\n",
      "Finished Training\n",
      "Accuracy of plane : 62 %\n",
      "Accuracy of   car : 51 %\n",
      "Accuracy of  bird : 31 %\n",
      "Accuracy of   cat : 23 %\n",
      "Accuracy of  deer : 41 %\n",
      "Accuracy of   dog : 54 %\n",
      "Accuracy of  frog : 56 %\n",
      "Accuracy of horse : 56 %\n",
      "Accuracy of  ship : 61 %\n",
      "Accuracy of truck : 54 %\n",
      "0.4924\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dfc4f3e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net4 = rand_Net4()\n",
    "net4.cuda()\n",
    "train_eval(net4,n_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg11 [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "   \n",
    "class rand_vgg11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rand_vgg11, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3,padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128*2, 3,padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128*2, 256*2, 3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(256*2, 256*2, 3,padding=1)\n",
    "                \n",
    "        self.conv5 = nn.Conv2d(256*2, 512*4, 3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(512*4, 512*4, 3,padding=1)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(512*4, 512*4, 3,padding=1)\n",
    "        self.conv8 = nn.Conv2d(512*4, 512*4, 3,padding=1)\n",
    "\n",
    "        self.fc = nn.Linear(512*4, 10)\n",
    "        for p in self.conv1.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv2.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv3.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv4.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv5.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv6.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv7.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.conv8.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2, stride=2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)),kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv6(x)),kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv8(x)),kernel_size=2, stride=2)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x, self.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netvgg11 = rand_vgg11()\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "losses=[]\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, fcw = netvgg11(inputs)\n",
    "                \n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, netvgg11.parameters()), lr=0.001, momentum=0.9)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #l1 = torch.abs(fcw).sum()\n",
    "        #print('CRIT', criterion(outputs, labels))\n",
    "        #print('L1', l1)\n",
    "        \n",
    "        loss = criterion(outputs, labels) #+ 0.001*l1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    losses.append(loss.data[0])\n",
    "plt.plot(losses)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane :  0 %\n",
      "Accuracy of   car :  0 %\n",
      "Accuracy of  bird :  0 %\n",
      "Accuracy of   cat :  0 %\n",
      "Accuracy of  deer : 100 %\n",
      "Accuracy of   dog :  0 %\n",
      "Accuracy of  frog :  0 %\n",
      "Accuracy of horse :  0 %\n",
      "Accuracy of  ship :  0 %\n",
      "Accuracy of truck :  0 %\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs,_ = netvgg11(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(8):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "print np.sum(class_correct)/np.sum(class_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to pixel by pixel Lasso classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=7000,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=500,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute purely on pixels\n",
      "1\n",
      "-----\n",
      "Accuracy of plane : 43 %\n",
      "Accuracy of   car : 43 %\n",
      "Accuracy of  bird : 26 %\n",
      "Accuracy of   cat : 17 %\n",
      "Accuracy of  deer : 28 %\n",
      "Accuracy of   dog : 27 %\n",
      "Accuracy of  frog : 48 %\n",
      "Accuracy of horse : 48 %\n",
      "Accuracy of  ship : 53 %\n",
      "Accuracy of truck : 41 %\n",
      "0.3804\n",
      "-----\n",
      "Accuracy of plane : 46 %\n",
      "Accuracy of   car : 46 %\n",
      "Accuracy of  bird : 33 %\n",
      "Accuracy of   cat : 21 %\n",
      "Accuracy of  deer : 32 %\n",
      "Accuracy of   dog : 36 %\n",
      "Accuracy of  frog : 51 %\n",
      "Accuracy of horse : 45 %\n",
      "Accuracy of  ship : 66 %\n",
      "Accuracy of truck : 51 %\n",
      "0.4368\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model,metrics,preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print('compute purely on pixels')\n",
    "for ty,data in enumerate(trainloader):\n",
    "    images, labels = data\n",
    "    for alpha in [1]:#np.logspace(0,3,10):\n",
    "        print alpha\n",
    "        reg = linear_model.LogisticRegression(penalty='l1',solver ='saga',C = alpha,n_jobs=6,max_iter=3)\n",
    "        rf = RandomForestClassifier(n_estimators = 300,n_jobs=6)\n",
    "        reg.fit(images[:,:,:,:].numpy().reshape(7000,-1) , labels) \n",
    "        rf.fit(images[:,:,:,:].numpy().reshape(7000,-1) , labels) \n",
    "    for mod in [reg,rf]:\n",
    "        print (\"-----\")\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        for a,data in enumerate(testloader):\n",
    "            imagesp, labelsp = data\n",
    "            predicted = mod.predict(imagesp[:,:,:,:].numpy().reshape(500,-1))\n",
    "            c = (predicted == labelsp).squeeze()\n",
    "            for i in range(500):\n",
    "                label = labelsp[i]\n",
    "                class_correct[label] += c[i]\n",
    "                class_total[label] += 1\n",
    "            if a>=4:\n",
    "                break \n",
    "        for i in range(10):\n",
    "            print('Accuracy of %5s : %2d %%' % (\n",
    "                classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "        print np.sum(class_correct)/np.sum(class_total)\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
